{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan age or birth age\n",
    "task = 'scan_age'\n",
    "\n",
    "model_name = 'MLP.pt'\n",
    "\n",
    "path = '/home/daniel/data/release/'\n",
    "\n",
    "# hyperparameters\n",
    "bs = 8\n",
    "lr = 0.001\n",
    "epochs = 200\n",
    "hidden = 64\n",
    "features = 'pos+norm+dha+x'\n",
    "\n",
    "in_channels = 0\n",
    "if 'pos' in features:\n",
    "    in_channels += 3\n",
    "if 'norm' in features:\n",
    "    in_channels += 3\n",
    "if 'dha' in features:\n",
    "    in_channels += 3\n",
    "if 'x' in features:\n",
    "    in_channels += 4\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from MLP import MLP, actor_MLP\n",
    "from GCN import GCN, actor_GCN\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00061XX04_ses-13300_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00084XX11_ses-31201_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00143AN12_ses-47501_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00170XX06_ses-56100_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00217XX11_ses-73700_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00221XX07_ses-75000_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00291XX12_ses-93100_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00307XX10_ses-98800_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00341XX12_ses-108000_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00319XX14_ses-117300_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/surfaces/sub-CC00439XX19_ses-132100_left.wm.surf.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00442XX14_ses-133300_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00371XX09_ses-134700_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00468XX15_ses-139100_left.shape.gii'\n",
      "No such file or no access: '/home/daniel/data/release/features/sub-CC00501XX06_ses-146500_left.shape.gii'\n"
     ]
    }
   ],
   "source": [
    "log_dir=f'runs/invase/{task}/{model_name}/features={features}/bs={bs}_lr={lr}_epoch={epochs}_hidden={hidden}'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_ids = pd.read_csv(task + '_train.txt', header=None)\n",
    "val_ids = pd.read_csv(task + '_val.txt', header=None)\n",
    "test_ids = pd.read_csv(task + '_test.txt', header=None)\n",
    "\n",
    "df = pd.read_csv(\"combined.tsv\", sep='\\t')\n",
    "\n",
    "df.insert(0, \"ID\", \"sub-\" + df[\"participant_id\"] + \"_\" + \"ses-\" + df[\"session_id\"].apply(str))\n",
    "df.drop(\"participant_id\", axis=1, inplace=True)\n",
    "df.drop(\"session_id\", axis=1, inplace=True)\n",
    "\n",
    "transform = T.Compose([T.NormalizeScale(), T.GenerateMeshNormals(), T.FaceToEdge()])\n",
    "\n",
    "def get_data(path, task, ids):\n",
    "    dataset = []\n",
    "    for _id in ids[0]:\n",
    "        try:\n",
    "            surface = nib.load(os.path.join(path, 'surfaces', _id + '_left.wm.surf.gii'))\n",
    "            pos, face = surface.agg_data()\n",
    "            feature = nib.load(os.path.join(path, 'features', _id + '_left.shape.gii'))\n",
    "            x = np.stack(feature.agg_data(), axis=1)\n",
    "            y = np.array([[df.loc[df['ID'] == _id, task].item()]])\n",
    "            data = Data()\n",
    "            data.id = _id\n",
    "            if 'x' in features:\n",
    "                data.x = torch.from_numpy(x).to(torch.float32)\n",
    "            data.pos = torch.from_numpy(pos).to(torch.float32)\n",
    "            data.face = torch.from_numpy(face.T).to(torch.long)\n",
    "            data.y = torch.from_numpy(y).to(torch.float32)\n",
    "            if task == 'birth_age':\n",
    "                confound = np.array([[df.loc[df['ID'] == _id, 'scan_age'].item()]])\n",
    "                data.confound = torch.from_numpy(confound).to(torch.float32)\n",
    "            data = transform(data)\n",
    "            if 'norm' not in features:\n",
    "                data.norm = None\n",
    "            if 'dha' in features:\n",
    "                data.dha = torch.from_numpy(np.load(os.path.join(path, 'preprocess/V_dihedral_angles', \\\n",
    "                                                                 _id + '_left.wm.surf_V_dihedralAngles.npy'))).to(torch.float32)\n",
    "            # data.eig = torch.from_numpy(np.load(os.path.join(path, 'preprocess/aligned_eigen_vectors',\n",
    "            #                                                     _id + '_left.wm.surf_eigen.npy'))).to(torch.float32)\n",
    "            # data.curv = torch.from_numpy(np.load(os.path.join(path, 'preprocess/gaussian_curvatures',\n",
    "            #                                                     _id + '_left.wm.surf_gaussian_curvature.npy'))).to(torch.float32).unsqueeze(1)\n",
    "            # data.curv = (data.curv - data.curv.min()) / (data.curv.max() - data.curv.min())\n",
    "            # data.hks = torch.from_numpy(np.load(os.path.join(path, 'preprocess/HKS',\n",
    "            #                                                 _id + '_left.wm.surf_hks.npy'))).to(torch.float32)\n",
    "            dataset.append(data)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    return dataset\n",
    "\n",
    "train_set = get_data(path, task, train_ids)\n",
    "val_set = get_data(path, task, val_ids)\n",
    "test_set = get_data(path, task, test_ids)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=bs)\n",
    "test_loader = DataLoader(test_set, batch_size=bs)\n",
    "\n",
    "class Invase():\n",
    "    def __init__(self):\n",
    "        if model_name == 'MLP.pt':\n",
    "            self.critic = MLP(in_channels=in_channels, hidden_channels=hidden, out_channels=1)\n",
    "            self.baseline = MLP(in_channels=in_channels, hidden_channels=hidden, out_channels=1)\n",
    "            self.actor = actor_MLP(in_channels=in_channels, hidden_channels=hidden, out_channels=in_channels)\n",
    "        elif model_name == 'GCN.pt':\n",
    "            self.critic = GCN(in_channels=in_channels, hidden_channels=hidden, out_channels=1)\n",
    "            self.baseline = GCN(in_channels=in_channels, hidden_channels=hidden, out_channels=1)\n",
    "            self.actor = actor_GCN(in_channels=in_channels, hidden_channels=hidden, out_channels=in_channels)\n",
    "        self.critic = self.critic.to(device)\n",
    "        self.critic.optimizer = torch.optim.AdamW(self.critic.parameters(), lr=lr)\n",
    "        self.critic.criterion = nn.MSELoss()\n",
    "        self.baseline = self.baseline.to(device)\n",
    "        self.baseline.optimizer = torch.optim.AdamW(self.baseline.parameters(), lr=lr)\n",
    "        self.baseline.criterion = nn.MSELoss()\n",
    "        self.actor = self.actor.to(device)\n",
    "        self.actor.optimizer = torch.optim.AdamW(self.actor.parameters(), lr=lr)\n",
    "        self.actor.criterion = self.actor_loss\n",
    "        self.lambda_ = 1.0\n",
    "\n",
    "    def actor_loss(self, actor_pred, actor_out, critic_out, baseline_out, y_true):\n",
    "        critic_loss = F.mse_loss(critic_out, y_true)\n",
    "        baseline_loss = F.mse_loss(baseline_out, y_true)\n",
    "        reward = -(critic_loss - baseline_loss)\n",
    "        # reward * BCE(actor_pred, actor_out) - lambda * ||actor_pred||\n",
    "        custom_actor_loss = reward * torch.sum(actor_out * torch.log(actor_pred + 1e-8) + \\\n",
    "                                               (1.0 - actor_out)* torch.log(1.0 - actor_pred + 1e-8), dim=1) - \\\n",
    "                                                self.lambda_ * torch.mean(actor_pred, dim=1)\n",
    "        custom_actor_loss = torch.mean(-custom_actor_loss)\n",
    "        return custom_actor_loss\n",
    "\n",
    "invase = Invase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader, invase):\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "    critic_accs = []\n",
    "    baseline_losses = []\n",
    "    baseline_accs = []\n",
    "    invase.baseline.eval()\n",
    "    invase.critic.eval()\n",
    "    invase.actor.eval()\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        # baseline testing\n",
    "        baseline_out = invase.baseline(data)\n",
    "        baseline_loss = invase.baseline.criterion(baseline_out, data.y)\n",
    "        baseline_losses.append(baseline_loss.item())\n",
    "        baseline_acc = F.l1_loss(baseline_out, data.y)\n",
    "        baseline_accs.append(baseline_acc.item())\n",
    "        # critic testing\n",
    "        selection_probability = invase.actor(data)\n",
    "        selection = torch.bernoulli(selection_probability)\n",
    "        critic_out = invase.critic(data, selection)\n",
    "        critic_loss = invase.critic.criterion(critic_out, data.y)\n",
    "        critic_losses.append(critic_loss.item())\n",
    "        critic_acc = F.l1_loss(critic_out, data.y)\n",
    "        critic_accs.append(critic_acc.item())\n",
    "        # actor testing\n",
    "        actor_loss = invase.actor.criterion(selection_probability, selection, critic_out, baseline_out, data.y)\n",
    "        actor_losses.append(actor_loss.item())\n",
    "        return sum(actor_losses) / len(actor_losses), \\\n",
    "               sum(critic_losses) / len(critic_losses), sum(critic_accs) / len(critic_accs), \\\n",
    "               sum(baseline_losses) / len(baseline_losses), sum(baseline_accs) / len(baseline_accs)\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_reg(loader, invase):\n",
    "    invase.baseline.eval()\n",
    "    invase.critic.eval()\n",
    "    invase.actor.eval()\n",
    "    baseline_outs = []\n",
    "    critic_outs = []\n",
    "    ys = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        baseline_out = invase.baseline(data)\n",
    "        baseline_outs.append(baseline_out.cpu().numpy())\n",
    "        selection_probability = invase.actor(data)\n",
    "        selection = torch.bernoulli(selection_probability)\n",
    "        critic_out = invase.critic(data, selection)\n",
    "        critic_outs.append(critic_out.cpu().numpy())\n",
    "        ys.append(data.y.cpu().numpy())\n",
    "    plt.scatter(np.concatenate(ys), np.concatenate(baseline_outs))\n",
    "    plt.xlabel('y')\n",
    "    plt.ylabel('baseline_out')\n",
    "    plt.savefig(os.path.join(log_dir, 'baseline_regression.png'))\n",
    "    plt.close()\n",
    "    plt.scatter(np.concatenate(ys), np.concatenate(critic_outs))\n",
    "    plt.xlabel('y')\n",
    "    plt.ylabel('critic_out')\n",
    "    plt.savefig(os.path.join(log_dir, 'critic_regression.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invase.baseline.load_state_dict(torch.load(os.path.join(log_dir, model_name + '_baseline')))\n",
    "invase.critic.load_state_dict(torch.load(os.path.join(log_dir, model_name + '_critic')))\n",
    "invase.actor.load_state_dict(torch.load(os.path.join(log_dir, model_name + '_actor')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reg(test_loader, invase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.2609210014343262, 0.23795820772647858, 0.3807697296142578, 0.4252564013004303, 0.5857276916503906)\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, invase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3159050710412253\n",
      "0.20948900853272173\n",
      "0.42358452059320884\n",
      "0.6075433924857626\n",
      "0.6763396761841824\n",
      "0.20141304981632296\n",
      "0.1989543042624735\n",
      "0.6563490058115804\n",
      "0.23016968259830123\n",
      "0.19404653151664755\n",
      "0.6469319131567183\n",
      "0.19336624618554296\n",
      "0.2192753989387549\n",
      "0.32662910115423066\n",
      "0.21629555575752069\n",
      "0.41898573116422794\n",
      "0.5927019903662637\n",
      "0.6687539761883122\n",
      "0.20852494774152505\n",
      "0.20318095064982278\n",
      "0.6483140961555939\n",
      "0.2368444969553758\n",
      "0.20145414886849042\n",
      "0.6326001999454695\n",
      "0.19931836771789513\n",
      "0.22564755066799963\n",
      "0.35042054518870247\n",
      "0.24155652387914936\n",
      "0.40092939762873314\n",
      "0.5562053939806303\n",
      "0.6499124165786007\n",
      "0.23211778160603386\n",
      "0.22920798529177586\n",
      "0.625519347974\n",
      "0.2617948086917498\n",
      "0.22330153307178946\n",
      "0.5952777335437266\n",
      "0.2222592179741448\n",
      "0.2523271132214775\n",
      "0.3410951379305725\n",
      "0.22281916960302256\n",
      "0.41121933573600933\n",
      "0.5759857447179089\n",
      "0.663044788917322\n",
      "0.21397660740363617\n",
      "0.21156499953107624\n",
      "0.6368654456785326\n",
      "0.24625195943139647\n",
      "0.20776001822103726\n",
      "0.6126421843808196\n",
      "0.20718391189592572\n",
      "0.23334985731320088\n",
      "0.3458955180745176\n",
      "0.23285897598927205\n",
      "0.40598591956348334\n",
      "0.5636798723743686\n",
      "0.6564743419303377\n",
      "0.22589967977989203\n",
      "0.22247783313873506\n",
      "0.633087869784863\n",
      "0.2545345248141683\n",
      "0.21817740425187565\n",
      "0.6054587702160619\n",
      "0.21669768678542942\n",
      "0.24307827474191646\n",
      "0.3393786167762234\n",
      "0.23155233440080902\n",
      "0.4062812517557166\n",
      "0.5726052025394686\n",
      "0.6581830439912355\n",
      "0.22081015787403788\n",
      "0.21851789426372267\n",
      "0.6364290128658914\n",
      "0.25304792404067644\n",
      "0.21228158885330636\n",
      "0.6165177818978594\n",
      "0.21313556941401202\n",
      "0.24060902297881903\n",
      "0.34133643585765994\n",
      "0.23492220185211302\n",
      "0.40935693132139334\n",
      "0.5767052131810276\n",
      "0.6580877168241432\n",
      "0.22414563061157336\n",
      "0.21972688384336952\n",
      "0.636135006816152\n",
      "0.25430122690734736\n",
      "0.21747050251492503\n",
      "0.6147934000846142\n",
      "0.21754101443143892\n",
      "0.24062191510365252\n",
      "0.3357498241029249\n",
      "0.22891747914363253\n",
      "0.410996080008041\n",
      "0.5801588099306463\n",
      "0.6643255603578249\n",
      "0.22085134184340136\n",
      "0.2151221228264147\n",
      "0.6389461252387174\n",
      "0.24770077394713036\n",
      "0.21292340938787818\n",
      "0.6211302643481756\n",
      "0.2113780279425068\n",
      "0.23753643582269574\n",
      "0.32683532373191115\n",
      "0.2195377064631441\n",
      "0.4138573119849197\n",
      "0.5896483948460363\n",
      "0.6664750979873795\n",
      "0.20921598602314917\n",
      "0.2072619854944196\n",
      "0.6420156090159883\n",
      "0.23848001747106354\n",
      "0.20333099619544603\n",
      "0.6299122998586223\n",
      "0.20436546706359698\n",
      "0.22796289697819566\n",
      "0.32760936300844207\n",
      "0.21931227082800375\n",
      "0.4171569881470112\n",
      "0.589131917796538\n",
      "0.6679244478553765\n",
      "0.21084889571075296\n",
      "0.20754455529973564\n",
      "0.6478852221369489\n",
      "0.24066257354822204\n",
      "0.20318495778971604\n",
      "0.6276221540035815\n",
      "0.20274793212245246\n",
      "0.2282979449134476\n",
      "0.34892127104875376\n",
      "0.24339097469741433\n",
      "0.40579856169473166\n",
      "0.5638358165719971\n",
      "0.6493658610309954\n",
      "0.23480144586203858\n",
      "0.23213138704517688\n",
      "0.626216954873487\n",
      "0.26179171022305064\n",
      "0.22463758989407928\n",
      "0.6004609629844204\n",
      "0.22628748472902682\n",
      "0.2520056927669114\n",
      "0.34539928156949434\n",
      "0.23442633017101103\n",
      "0.39911270762334594\n",
      "0.5618802001780725\n",
      "0.6550458997267508\n",
      "0.22472444812870346\n",
      "0.22199195603450922\n",
      "0.6267692118755949\n",
      "0.2520493690706457\n",
      "0.2150532682447576\n",
      "0.6038654017377422\n",
      "0.2167572380338338\n",
      "0.24276196616622148\n",
      "0.35218041748808077\n",
      "0.24604672622792667\n",
      "0.39748053073836387\n",
      "0.5643639109097772\n",
      "0.6490219929253527\n",
      "0.2381751324748682\n",
      "0.23582623771374242\n",
      "0.623953133956909\n",
      "0.265760664401661\n",
      "0.2289613131440236\n",
      "0.6024495616794597\n",
      "0.2297722410972694\n",
      "0.256630734169428\n",
      "0.3387240372803805\n",
      "0.2299370020172878\n",
      "0.40754848816111255\n",
      "0.5718616523728489\n",
      "0.6577597962426606\n",
      "0.22016611632650762\n",
      "0.2187573957828542\n",
      "0.635287886130301\n",
      "0.25158621933215375\n",
      "0.21396774593443252\n",
      "0.6089053677887596\n",
      "0.21257029515512832\n",
      "0.23962899935762344\n",
      "0.34019582613384314\n",
      "0.22741810962832248\n",
      "0.4036493756356997\n",
      "0.5713515434481575\n",
      "0.6610663823633935\n",
      "0.21892577478769162\n",
      "0.2162420499246351\n",
      "0.635564868938642\n",
      "0.24985601019570358\n",
      "0.2101515875641827\n",
      "0.6114235996225629\n",
      "0.21197750082717548\n",
      "0.23669472936044017\n",
      "0.3472234169359893\n",
      "0.23840888703196234\n",
      "0.4023495582289836\n",
      "0.5565454612482642\n",
      "0.6529609103309288\n",
      "0.22845521461838111\n",
      "0.22674711527827274\n",
      "0.6242672991140003\n",
      "0.2570013640217752\n",
      "0.2192880052103174\n",
      "0.5959300539464468\n",
      "0.22050456517197733\n",
      "0.24662988313651277\n",
      "0.35588602239972467\n",
      "0.24527841911290207\n",
      "0.39969598325039796\n",
      "0.5586020965683393\n",
      "0.6482153356373597\n",
      "0.2350250240201913\n",
      "0.23420762049531785\n",
      "0.6258873130368692\n",
      "0.26261597808784937\n",
      "0.22887298696456485\n",
      "0.598826954239743\n",
      "0.22910243356803808\n",
      "0.2528644974402363\n",
      "0.3468974292455744\n",
      "0.23827348083361127\n",
      "0.40383451308848783\n",
      "0.5657512754944471\n",
      "0.6578462982865755\n",
      "0.22795834414260832\n",
      "0.22537647160557883\n",
      "0.6341153071687111\n",
      "0.25577832956552893\n",
      "0.22031155418843965\n",
      "0.6063817959455954\n",
      "0.21900208773425242\n",
      "0.24391901073515423\n",
      "0.3314570910821126\n",
      "0.22231445955111917\n",
      "0.41692060053250185\n",
      "0.5843144797983377\n",
      "0.6693021796130757\n",
      "0.213709391672319\n",
      "0.20836412597819376\n",
      "0.6463013393535063\n",
      "0.24048633818929124\n",
      "0.20554976260136265\n",
      "0.6234017351866288\n",
      "0.20395023233683274\n",
      "0.2316585509065692\n",
      "0.3131200453518886\n",
      "0.20726544570117128\n",
      "0.41891064032112063\n",
      "0.6076513024952682\n",
      "0.6765294832993499\n",
      "0.20004206021926174\n",
      "0.19807620214507118\n",
      "0.6579864127204733\n",
      "0.2279663883986943\n",
      "0.19264494774474933\n",
      "0.6485320069125056\n",
      "0.19318441577441092\n",
      "0.21863084842777072\n",
      "0.33697219606955153\n",
      "0.22414758845892574\n",
      "0.4065952400815169\n",
      "0.573323481390554\n",
      "0.6651372320013348\n",
      "0.2155787817754949\n",
      "0.212313339451072\n",
      "0.6384654804609756\n",
      "0.2443599613867404\n",
      "0.20687888069217844\n",
      "0.6097438892133145\n",
      "0.2072721638918352\n",
      "0.23378898568687506\n",
      "0.3466436002447865\n",
      "0.24103234006496257\n",
      "0.40297980511227227\n",
      "0.5636091889092878\n",
      "0.650331874029092\n",
      "0.23168808548698394\n",
      "0.22925198889045803\n",
      "0.6280304100174176\n",
      "0.2599326837075743\n",
      "0.2206609235983618\n",
      "0.60263380878407\n",
      "0.22188485618792073\n",
      "0.2509061808595773\n",
      "0.3272968597600829\n",
      "0.2118329629666871\n",
      "0.41162984786477763\n",
      "0.5853837568249691\n",
      "0.6711948597801932\n",
      "0.20388935254547466\n",
      "0.2028134458175383\n",
      "0.6488421433670853\n",
      "0.23573418065177826\n",
      "0.19665966154186484\n",
      "0.6267709726397924\n",
      "0.197343415350273\n",
      "0.2243617459854602\n",
      "0.33439754868043886\n",
      "0.22153800533755066\n",
      "0.41093209449441537\n",
      "0.5822477018879114\n",
      "0.6642779480083029\n",
      "0.21187110803597906\n",
      "0.20923198576653157\n",
      "0.6398635959276465\n",
      "0.23842048037955915\n",
      "0.20417119699515668\n",
      "0.6232084610062272\n",
      "0.203884550756153\n",
      "0.23031531086290402\n",
      "0.3522028115993177\n",
      "0.23862125757308394\n",
      "0.4028821834009764\n",
      "0.5592847479560026\n",
      "0.6509499441209341\n",
      "0.228621845773778\n",
      "0.22645726721957532\n",
      "0.6255161461090524\n",
      "0.25931415799070645\n",
      "0.22052820422328098\n",
      "0.6027410152343979\n",
      "0.22375154402682196\n",
      "0.24810305276160227\n",
      "0.32247293331984217\n",
      "0.20767985429525448\n",
      "0.4193058787817464\n",
      "0.5960133562683396\n",
      "0.675250430031367\n",
      "0.19913993726601234\n",
      "0.1969442476980674\n",
      "0.6542547809369624\n",
      "0.22607507841748456\n",
      "0.19207730446220783\n",
      "0.6336739856318931\n",
      "0.19110593949205706\n",
      "0.21846605281797024\n",
      "0.33451476793248947\n",
      "0.22443362544628367\n",
      "0.406867900032457\n",
      "0.5774618630314833\n",
      "0.6632002596559559\n",
      "0.21351509250243428\n",
      "0.212359623498864\n",
      "0.6387406686140863\n",
      "0.24446608244076598\n",
      "0.20662122687439144\n",
      "0.6148003894839338\n",
      "0.2044790652385589\n",
      "0.23363842908146706\n",
      "0.33528091301976254\n",
      "0.22289472580036085\n",
      "0.41085889313137247\n",
      "0.5743716695338397\n",
      "0.6641106868627533\n",
      "0.21328619980699032\n",
      "0.21046448202072757\n",
      "0.6394914614190408\n",
      "0.24299290899173415\n",
      "0.2037091427852138\n",
      "0.6159946292955146\n",
      "0.20476859816221205\n",
      "0.23188436201904922\n",
      "0.3170121188070454\n",
      "0.2047274540329884\n",
      "0.42055618303367015\n",
      "0.6009137438057734\n",
      "0.6777577663135831\n",
      "0.1951117758987352\n",
      "0.19400266593404358\n",
      "0.654975223093908\n",
      "0.22624825747631694\n",
      "0.19044130366209794\n",
      "0.6392238265311314\n",
      "0.18850799271447033\n",
      "0.21580837835912778\n",
      "0.331778960895906\n",
      "0.21810170736185056\n",
      "0.4109142647328805\n",
      "0.5820084450156049\n",
      "0.6663392693225629\n",
      "0.20973012667523408\n",
      "0.2082430695795851\n",
      "0.6433633192583074\n",
      "0.23826877180099137\n",
      "0.20271709197723517\n",
      "0.619083899394162\n",
      "0.2011015237745548\n",
      "0.2292179181200661\n",
      "0.32826875357892726\n",
      "0.21068906279824395\n",
      "0.4148883374689826\n",
      "0.5891296048864287\n",
      "0.6701660622256156\n",
      "0.2033975949608704\n",
      "0.20135522046192023\n",
      "0.6448368009162054\n",
      "0.23021569001717884\n",
      "0.19317617866004963\n",
      "0.6234491315136477\n",
      "0.19474136285550678\n",
      "0.2211299866386715\n",
      "0.33258026159334125\n",
      "0.22762584225128815\n",
      "0.4112960760998811\n",
      "0.5836107808164883\n",
      "0.6628319460959176\n",
      "0.21893579072532698\n",
      "0.2151407055093143\n",
      "0.6423900118906064\n",
      "0.24700753071739992\n",
      "0.20944312326595324\n",
      "0.6237217598097503\n",
      "0.21006738010305193\n",
      "0.24021997621878716\n",
      "0.331351928059747\n",
      "0.22007887517146776\n",
      "0.41496151501295536\n",
      "0.5857624599908551\n",
      "0.6674763755525073\n",
      "0.21162932479804908\n",
      "0.20844764517604022\n",
      "0.6435089925316263\n",
      "0.24037875323883554\n",
      "0.20305593659503124\n",
      "0.6236949397957628\n",
      "0.20269394909312605\n",
      "0.2281759640298735\n",
      "0.3199709087751292\n",
      "0.21214884286396596\n",
      "0.4185445703616864\n",
      "0.6006143893013765\n",
      "0.6762732838348313\n",
      "0.20348660501063784\n",
      "0.1991446311493205\n",
      "0.6562567843341583\n",
      "0.23041769788545874\n",
      "0.19440102470583126\n",
      "0.6414940732056793\n",
      "0.19457470366028395\n",
      "0.22031175372324258\n",
      "0.3130395822922231\n",
      "0.2020902621012907\n",
      "0.42472988575071946\n",
      "0.6079509227826506\n",
      "0.681144560665851\n",
      "0.19414623218624527\n",
      "0.1905274766934914\n",
      "0.6624304251176095\n",
      "0.2239061880718926\n",
      "0.18615050576416053\n",
      "0.6509021040478365\n",
      "0.18621081835570644\n",
      "0.20904344229808206\n",
      "0.3308244953814574\n",
      "0.21902915573801648\n",
      "0.4157923746531341\n",
      "0.5860227315923519\n",
      "0.6683297221271904\n",
      "0.2101341848177291\n",
      "0.2086801991865283\n",
      "0.6459687535636903\n",
      "0.23925191013798608\n",
      "0.2035674915421751\n",
      "0.6227144866385372\n",
      "0.2029402820542061\n",
      "0.2311932185349907\n",
      "0.3117671853897305\n",
      "0.19564375854860772\n",
      "0.4268067132915258\n",
      "0.6073705127874268\n",
      "0.6829251507565787\n",
      "0.1871704216022779\n",
      "0.18484778101800475\n",
      "0.6595869140709008\n",
      "0.21713248513940145\n",
      "0.18107994184796167\n",
      "0.6457198895455366\n",
      "0.17924763649814618\n",
      "0.20723115435237038\n",
      "0.32638870147978233\n",
      "0.21622812935726174\n",
      "0.41537354383124186\n",
      "0.5878558898934017\n",
      "0.6718841361939459\n",
      "0.20665677146583908\n",
      "0.2050195655107273\n",
      "0.6492601088472091\n",
      "0.23805154500067469\n",
      "0.20023388656501598\n",
      "0.6290559078846759\n",
      "0.19919039265956012\n",
      "0.2263572167498763\n",
      "0.3359016026000224\n",
      "0.2195356569165826\n",
      "0.41234263513765923\n",
      "0.5794855990137846\n",
      "0.6668347715641226\n",
      "0.21046733161492773\n",
      "0.2074414434607195\n",
      "0.6431319810228249\n",
      "0.2385968844559005\n",
      "0.2025383839515858\n",
      "0.6165900855467145\n",
      "0.2009320482647839\n",
      "0.2289121745302402\n",
      "0.33698177644654703\n",
      "0.22288161688547994\n",
      "0.41437355951814425\n",
      "0.5796265711166475\n",
      "0.6665173697617781\n",
      "0.21403577527083392\n",
      "0.21192695648928328\n",
      "0.6399425206916179\n",
      "0.24160904739243624\n",
      "0.20598307346340827\n",
      "0.6212524144108837\n",
      "0.20586176972818632\n",
      "0.23029047579057377\n",
      "0.3422866466142047\n",
      "0.23273804899164235\n",
      "0.41016904003462934\n",
      "0.5739480781824035\n",
      "0.6589231605935825\n",
      "0.2217831892293861\n",
      "0.22002952373552948\n",
      "0.6367248632029924\n",
      "0.25068537243193445\n",
      "0.2150904025661232\n",
      "0.6173346504323118\n",
      "0.2130481592061889\n",
      "0.24014118117140415\n",
      "0.32964812983736236\n",
      "0.22003818745951106\n",
      "0.4180333458351802\n",
      "0.5873367656585632\n",
      "0.6684339732005864\n",
      "0.21277575096321047\n",
      "0.20910191278257015\n",
      "0.6459391728323503\n",
      "0.2398564560673736\n",
      "0.20449896007364723\n",
      "0.6244075829383886\n",
      "0.20341641378840056\n",
      "0.23073579051450782\n",
      "0.38881936756990904\n",
      "0.2831496366174135\n",
      "0.3702892621648939\n",
      "0.5048370794628676\n",
      "0.6106752659190451\n",
      "0.27321076190017807\n",
      "0.2724647446695866\n",
      "0.5936131299032584\n",
      "0.30365307792270296\n",
      "0.264691726428262\n",
      "0.5531356788756798\n",
      "0.26611156567358135\n",
      "0.2932088366944217\n",
      "0.38539113232701155\n",
      "0.28398486216832236\n",
      "0.37496237044682407\n",
      "0.5152668472885219\n",
      "0.616436588827248\n",
      "0.2744807121661721\n",
      "0.27155635831935665\n",
      "0.5931707736636133\n",
      "0.3003913473530297\n",
      "0.2646970283404292\n",
      "0.555304691867716\n",
      "0.26572915322754054\n",
      "0.29503719950113966\n",
      "0.4342442833240379\n",
      "0.3297824874511991\n",
      "0.32989403234802006\n",
      "0.44651422197434465\n",
      "0.5698271054099275\n",
      "0.32052426101505854\n",
      "0.320580033463469\n",
      "0.5350250976017847\n",
      "0.35158951477969885\n",
      "0.31143335192414945\n",
      "0.49040713887339654\n",
      "0.3176798661461238\n",
      "0.34288901282766315\n",
      "0.35983928408364535\n",
      "0.2559583599671263\n",
      "0.39680394484521964\n",
      "0.5457035887133596\n",
      "0.6366176604876267\n",
      "0.24286366541868323\n",
      "0.24043466350105014\n",
      "0.6172586978358141\n",
      "0.2750068486896174\n",
      "0.2369098712446352\n",
      "0.5831248287827596\n",
      "0.2353027120810885\n",
      "0.26399415578485985\n",
      "0.42162489894907035\n",
      "0.322554567502021\n",
      "0.34373484236054974\n",
      "0.47146321746160064\n",
      "0.5786580436540016\n",
      "0.311519805982215\n",
      "0.3042037186742118\n",
      "0.5486257073565077\n",
      "0.34365400161681486\n",
      "0.3018997574777688\n",
      "0.5083670169765562\n",
      "0.3021018593371059\n",
      "0.3313257881972514\n",
      "0.40648586092382005\n",
      "0.30987722652712085\n",
      "0.369326758579048\n",
      "0.500654120962061\n",
      "0.598847740766831\n",
      "0.2960903693267586\n",
      "0.2935745194726779\n",
      "0.5802304518466338\n",
      "0.3237647177216464\n",
      "0.2877880648082922\n",
      "0.5409580356244339\n",
      "0.2887189292543021\n",
      "0.3170222401127101\n",
      "0.3672953081876725\n",
      "0.26709008562734415\n",
      "0.39220508102752816\n",
      "0.5371700516594721\n",
      "0.6318024202108838\n",
      "0.25238836600382136\n",
      "0.2512561036020098\n",
      "0.6101832849762933\n",
      "0.28552473285683955\n",
      "0.2467447455947916\n",
      "0.5806913877291062\n",
      "0.24633783879414053\n",
      "0.271583752034534\n",
      "0.3683362473859234\n",
      "0.265607322842744\n",
      "0.38979738763756044\n",
      "0.5361171106311495\n",
      "0.6363262367582022\n",
      "0.2546710548870376\n",
      "0.25177414378278307\n",
      "0.6121053172889026\n",
      "0.2853714559978059\n",
      "0.24476327608077067\n",
      "0.5762967534025849\n",
      "0.2443347389351709\n",
      "0.27251534162981245\n",
      "0.3852184432140697\n",
      "0.2830921695435723\n",
      "0.3782626901781976\n",
      "0.5116316940399199\n",
      "0.6216907830456427\n",
      "0.2705997301446983\n",
      "0.2685292886055925\n",
      "0.5978690736518867\n",
      "0.3023077280975201\n",
      "0.2611315311962034\n",
      "0.5541339040617875\n",
      "0.2619457497789978\n",
      "0.29249057832782765\n",
      "0.36278810265881717\n",
      "0.25543499172726924\n",
      "0.38872215167955676\n",
      "0.5396898687906422\n",
      "0.640694139828389\n",
      "0.2455846704374928\n",
      "0.2430066566624341\n",
      "0.6176074493054754\n",
      "0.27600138520143136\n",
      "0.2391011581823079\n",
      "0.583381430605256\n",
      "0.23611912732309823\n",
      "0.2641117395821309\n",
      "0.3762044086334462\n",
      "0.27407668387432793\n",
      "0.387677884836323\n",
      "0.5287449640783689\n",
      "0.626292489392013\n",
      "0.2600143992892266\n",
      "0.25885020143686527\n",
      "0.6038050887700862\n",
      "0.2893184847045848\n",
      "0.2530904857462355\n",
      "0.5720653789004457\n",
      "0.25203351664343376\n",
      "0.28187374580658997\n",
      "0.42338448238599524\n",
      "0.3148044089042576\n",
      "0.34424032850659175\n",
      "0.46652258482818243\n",
      "0.5811973200778042\n",
      "0.30568402852820403\n",
      "0.30914199265182624\n",
      "0.5443700021612276\n",
      "0.3400475470066998\n",
      "0.2980332829046899\n",
      "0.5053814566673871\n",
      "0.29937324400259346\n",
      "0.3291981845688351\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for data in test_set:\n",
    "        # save surface\n",
    "        surface = nib.load(os.path.join(path, 'surfaces', data.id + '_left.wm.surf.gii'))\n",
    "        nib.save(surface, os.path.join('selection', data.id + '_left.wm.surf.gii'))\n",
    "        # evaluate model\n",
    "        invase.baseline.eval()\n",
    "        invase.critic.eval()\n",
    "        invase.actor.eval()\n",
    "        data = data.to(device)\n",
    "        baseline_out = invase.baseline(data)\n",
    "        selection_probability = invase.actor(data)\n",
    "        selection = torch.bernoulli(selection_probability)\n",
    "        critic_out = invase.critic(data, selection)\n",
    "        # save selection\n",
    "        feature = nib.load(os.path.join(path, 'features', data.id + '_left.shape.gii'))\n",
    "        for i in range(3, -1, -1):\n",
    "            feature.remove_gifti_data_array(i)\n",
    "        for darray in selection.cpu().numpy().T:\n",
    "            gifti_data_array = nib.gifti.gifti.GiftiDataArray(data=darray, intent='NIFTI_INTENT_LABEL')\n",
    "            feature.add_gifti_data_array(gifti_data_array)\n",
    "            print(sum(darray) / len(darray))\n",
    "        nib.save(feature, os.path.join('selection', data.id + '_left.shape.gii'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
